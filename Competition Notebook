{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment Setup","metadata":{}},{"cell_type":"code","source":"! pip install -Uqq fastbook waterfallcharts treeinterpreter dtreeviz==1.4.1\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nimport numpy\nimport pandas\nimport fastbook\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom treeinterpreter import treeinterpreter\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.inspection import PartialDependenceDisplay\nfastbook.setup_book()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Import & Exploration","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-09-12T14:18:38.390895Z","iopub.execute_input":"2024-09-12T14:18:38.391443Z","iopub.status.idle":"2024-09-12T14:18:38.905379Z","shell.execute_reply.started":"2024-09-12T14:18:38.391366Z","shell.execute_reply":"2024-09-12T14:18:38.904108Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf_raw = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\nprint(\"df Shape: \",df_raw.shape)\nprint(df_raw.columns)\ndf_raw.describe()","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:17:32.757984Z","iopub.execute_input":"2024-09-12T23:17:32.758443Z","iopub.status.idle":"2024-09-12T23:17:32.912346Z","shell.execute_reply.started":"2024-09-12T23:17:32.758402Z","shell.execute_reply":"2024-09-12T23:17:32.910970Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\ndf Shape:  (1460, 81)\nIndex(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n       'SaleCondition', 'SalePrice'],\n      dtype='object')\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\ncount  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \nmean    730.500000    56.897260    70.049958   10516.828082     6.099315   \nstd     421.610009    42.300571    24.284752    9981.264932     1.382997   \nmin       1.000000    20.000000    21.000000    1300.000000     1.000000   \n25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \nmax    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n\n       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\ncount  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \nmean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \nstd       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \nmin       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \nmax       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n\n        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\ncount  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \nmean     94.244521    46.660274      21.954110     3.409589    15.060959   \nstd     125.338794    66.256028      61.119149    29.317331    55.757415   \nmin       0.000000     0.000000       0.000000     0.000000     0.000000   \n25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n75%     168.000000    68.000000       0.000000     0.000000     0.000000   \nmax     857.000000   547.000000     552.000000   508.000000   480.000000   \n\n          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \ncount  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \nmean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \nstd      40.177307    496.123024     2.703626     1.328095   79442.502883  \nmin       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \nmax     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n\n[8 rows x 38 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>...</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1201.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1452.000000</td>\n      <td>1460.000000</td>\n      <td>...</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>730.500000</td>\n      <td>56.897260</td>\n      <td>70.049958</td>\n      <td>10516.828082</td>\n      <td>6.099315</td>\n      <td>5.575342</td>\n      <td>1971.267808</td>\n      <td>1984.865753</td>\n      <td>103.685262</td>\n      <td>443.639726</td>\n      <td>...</td>\n      <td>94.244521</td>\n      <td>46.660274</td>\n      <td>21.954110</td>\n      <td>3.409589</td>\n      <td>15.060959</td>\n      <td>2.758904</td>\n      <td>43.489041</td>\n      <td>6.321918</td>\n      <td>2007.815753</td>\n      <td>180921.195890</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>421.610009</td>\n      <td>42.300571</td>\n      <td>24.284752</td>\n      <td>9981.264932</td>\n      <td>1.382997</td>\n      <td>1.112799</td>\n      <td>30.202904</td>\n      <td>20.645407</td>\n      <td>181.066207</td>\n      <td>456.098091</td>\n      <td>...</td>\n      <td>125.338794</td>\n      <td>66.256028</td>\n      <td>61.119149</td>\n      <td>29.317331</td>\n      <td>55.757415</td>\n      <td>40.177307</td>\n      <td>496.123024</td>\n      <td>2.703626</td>\n      <td>1.328095</td>\n      <td>79442.502883</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>20.000000</td>\n      <td>21.000000</td>\n      <td>1300.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1872.000000</td>\n      <td>1950.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2006.000000</td>\n      <td>34900.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>365.750000</td>\n      <td>20.000000</td>\n      <td>59.000000</td>\n      <td>7553.500000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>1954.000000</td>\n      <td>1967.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>2007.000000</td>\n      <td>129975.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>730.500000</td>\n      <td>50.000000</td>\n      <td>69.000000</td>\n      <td>9478.500000</td>\n      <td>6.000000</td>\n      <td>5.000000</td>\n      <td>1973.000000</td>\n      <td>1994.000000</td>\n      <td>0.000000</td>\n      <td>383.500000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>25.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.000000</td>\n      <td>2008.000000</td>\n      <td>163000.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1095.250000</td>\n      <td>70.000000</td>\n      <td>80.000000</td>\n      <td>11601.500000</td>\n      <td>7.000000</td>\n      <td>6.000000</td>\n      <td>2000.000000</td>\n      <td>2004.000000</td>\n      <td>166.000000</td>\n      <td>712.250000</td>\n      <td>...</td>\n      <td>168.000000</td>\n      <td>68.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>8.000000</td>\n      <td>2009.000000</td>\n      <td>214000.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1460.000000</td>\n      <td>190.000000</td>\n      <td>313.000000</td>\n      <td>215245.000000</td>\n      <td>10.000000</td>\n      <td>9.000000</td>\n      <td>2010.000000</td>\n      <td>2010.000000</td>\n      <td>1600.000000</td>\n      <td>5644.000000</td>\n      <td>...</td>\n      <td>857.000000</td>\n      <td>547.000000</td>\n      <td>552.000000</td>\n      <td>508.000000</td>\n      <td>480.000000</td>\n      <td>738.000000</td>\n      <td>15500.000000</td>\n      <td>12.000000</td>\n      <td>2010.000000</td>\n      <td>755000.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 38 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Random Forest with SciKit","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import RandomForestRegressor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_raw.copy()\ny = np.log(df['SalePrice'])\nX = df.drop('SalePrice',axis=1)\nprint(\"Total Features:\",len(X.columns))\nnumeric_features = X.select_dtypes(include=['number']).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\nprint(\"Numeric:\", len(numeric_features),\"Categorical\",len(categorical_features))","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:17:51.673295Z","iopub.execute_input":"2024-09-12T23:17:51.673719Z","iopub.status.idle":"2024-09-12T23:17:51.692071Z","shell.execute_reply.started":"2024-09-12T23:17:51.673679Z","shell.execute_reply":"2024-09-12T23:17:51.690787Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Total Features: 80\nNumeric: 37 Categorical 43\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"numeric_transformer = SimpleImputer(strategy='constant')\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\nmodel_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n])\n\nX_train, X_valid, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel_pipeline.fit(X_train, y_train)\ny_pred = model_pipeline.predict(X_valid)\n\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n\n#Gradient Boosting\nmodel_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', GradientBoostingRegressor(n_estimators=100, random_state=42))\n])\n\n\nX_train, x_valid, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel_pipeline.fit(X_train, y_train)\ny_pred = model_pipeline.predict(X_valid)\n\n\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(f\"Root Mean Squared Error (RMSE) with Gradient Boosting: {rmse}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-13T00:02:13.168440Z","iopub.execute_input":"2024-09-13T00:02:13.169233Z","iopub.status.idle":"2024-09-13T00:02:25.203356Z","shell.execute_reply.started":"2024-09-13T00:02:13.169175Z","shell.execute_reply":"2024-09-13T00:02:25.201994Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Root Mean Squared Error (RMSE): 0.1446557219782972\nRoot Mean Squared Error (RMSE) with Gradient Boosting: 0.13825013833442756\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"param_dist = {\n    'model__n_estimators': [100, 200, 300, 400],\n    'model__learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'model__max_depth': [3, 4, 5, 6],\n    'model__min_samples_split': [2, 5, 10],\n    'model__min_samples_leaf': [1, 2, 4],\n    'model__subsample': [0.8, 0.9, 1.0]\n}\n\nrandom_search = RandomizedSearchCV(\n    estimator=model_pipeline,\n    param_distributions=param_dist,\n    n_iter=20,  \n    cv=5,  \n    verbose=1,\n    random_state=42,\n    n_jobs=-1,  \n    scoring='neg_mean_squared_error'\n)\n\nrandom_search.fit(X_train, y_train)\n\n\nbest_model = random_search.best_estimator_\nprint(f\"Best parameters found: {random_search.best_params_}\")\n\ny_pred = best_model.predict(X_valid)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(f\"Root Mean Squared Error (RMSE) with tuned Gradient Boosting: {rmse}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-13T00:10:47.624480Z","iopub.execute_input":"2024-09-13T00:10:47.624971Z","iopub.status.idle":"2024-09-13T00:13:43.937120Z","shell.execute_reply.started":"2024-09-13T00:10:47.624923Z","shell.execute_reply":"2024-09-13T00:13:43.935678Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 20 candidates, totalling 100 fits\nBest parameters found: {'model__subsample': 0.9, 'model__n_estimators': 300, 'model__min_samples_split': 10, 'model__min_samples_leaf': 2, 'model__max_depth': 3, 'model__learning_rate': 0.1}\nRoot Mean Squared Error (RMSE) with tuned Gradient Boosting: 0.1312542849908379\n","output_type":"stream"}]},{"cell_type":"code","source":"X_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\ny_pred_test = best_model.predict(X_test)\nsubmission = pd.DataFrame({\n    'Id': X_test['Id'], \n    'SalePrice': np.exp(y_pred_test)\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file 'submission.csv' created successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-13T00:14:58.727991Z","iopub.execute_input":"2024-09-13T00:14:58.729116Z","iopub.status.idle":"2024-09-13T00:14:58.808427Z","shell.execute_reply.started":"2024-09-13T00:14:58.729054Z","shell.execute_reply":"2024-09-13T00:14:58.807112Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Submission file 'submission.csv' created successfully.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"df = df_raw.copy()\ndf['SalePrice'] = np.log(df['SalePrice'])\nprocs = [Categorify, FillMissing]\ncont,cat = cont_cat_split(df, 1, dep_var='SalePrice')\nsplits = RandomSplitter(valid_pct=0.3, seed=42)(range_of(df))\nprint(\"Continuous:\",len(cont), \"Categorical:\",len(cat))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = TabularDataLoaders.from_df(df, procs=procs, cat_names=cat, cont_names=cont, y_names='SalePrice', splits=splits, bs=64)\nprint(\"Train:\", len(dls.train.items), \"Valid:\", len(dls.valid.items))\ndls.show(3)\ndls.items.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Model","metadata":{}},{"cell_type":"code","source":"def rf(xs, y, n_estimators=250, max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_features=max_features,\n                                min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs, y, valid_xs, valid_y = dls.train.xs, dls.train.y, dls.valid.xs, dls.valid.y\nm = rf(xs, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"def r_mse(pred,y): \n    return round(math.sqrt(((pred-y)**2).mean()), 6)\n\ndef m_rmse(m, xs, y): \n    return r_mse(m.predict(xs), y)\n\ndef evaluate_cross_val(m, xs, y):\n    scores = cross_val_score(m, xs, y, cv=5, scoring='neg_mean_squared_error')\n    rmse_scores = np.sqrt(-scores)\n    print(\"Cross-validation RMSE scores:\", rmse_scores)\n    print(\"Mean RMSE:\", rmse_scores.mean())\n    print(\"Standard deviation of RMSE:\", rmse_scores.std())\n    \ndef evaluate_model(m, xs, y, valid_xs, valid_y):\n    print(\"Training Set RMSE:\",m_rmse(m, xs, y))\n    evaluate_cross_val(m, xs, y)\n    print(\"_______________________________________________________________\")\n    print(\"OOB Error:\",r_mse(m.oob_prediction_, y))\n    print(\"Validation Set RMSE:\",m_rmse(m, valid_xs, valid_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(m, xs, y, valid_xs, valid_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Model Imputation","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Evaluate Feature Importance","metadata":{}},{"cell_type":"code","source":"def rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi[fi['cols'] == 'SaleCondition']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi = rf_feat_importance(m, xs)\n\nplot_fi(fi[:21]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_keep = fi[fi.imp>0.0045].cols\nlen(to_keep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs = xs[to_keep]\nvalid_xs = valid_xs[to_keep]\nm = rf(xs, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(m, xs, y, valid_xs, valid_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Investigate Most Important Features","metadata":{}},{"cell_type":"markdown","source":"#### OverallQual (Overall material and finish quality)\nChanges Attempted:\n1. Change Cont - Cat\n2. Square value\n3. Custom bins\n","metadata":{}},{"cell_type":"code","source":"df = df_raw.copy()\ndf['SalePrice'] = np.log(df['SalePrice'])\ndf['OverallQual_cat'] = pd.cut(df['OverallQual'], bins=[0,4,7,10], labels=['Poor', 'Ok','Great'])\nprocs = [Categorify, FillMissing]\ncont,cat = cont_cat_split(df, 1, dep_var='SalePrice')\nsplits = RandomSplitter(valid_pct=0.2, seed=42)(range_of(df))\ndls = TabularDataLoaders.from_df(df, procs=procs, cat_names=cat, cont_names=cont, y_names='SalePrice', splits=splits, bs=64)\nxs, y, valid_xs, valid_y = dls.train.xs, dls.train.y, dls.valid.xs, dls.valid.y\nm = rf(xs, y)\nevaluate_model(m, xs, y, valid_xs, valid_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['OverallQual'].hist()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PartialDependenceDisplay.from_estimator(m, valid_xs, ['OverallQual']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### GrLivingArea (Above ground square footage)","metadata":{}},{"cell_type":"code","source":"df['GrLivArea'].min(), df['GrLivArea'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['GrLivArea'].hist(bins=30)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PartialDependenceDisplay.from_estimator(m, valid_xs, ['GrLivArea']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Year Built","metadata":{}},{"cell_type":"code","source":"df['YearBuilt'].hist(bins= 20)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PartialDependenceDisplay.from_estimator(m, valid_xs, ['YearBuilt','YearRemodAdd']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['YearRemodAdd'].max(), df['YearRemodAdd'].min())\nprint(df['YearRemodAdd'][df['YearRemodAdd'].isnull()])\ndf['YearRemodAdd'].hist(bins= 50)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_raw.copy()\ndf['SalePrice'] = np.log(df['SalePrice'])\ndf['Score'] = (df['OverallQual']*df['OverallCond'])\nprocs = [Categorify, FillMissing]\ncont,cat = cont_cat_split(df, 1, dep_var='SalePrice')\nsplits = RandomSplitter(valid_pct=0.2, seed=42)(range_of(df))\ndls = TabularDataLoaders.from_df(df, procs=procs, cat_names=cat, cont_names=cont, y_names='SalePrice', splits=splits, bs=64)\nxs, y, valid_xs, valid_y = dls.train.xs, dls.train.y, dls.valid.xs, dls.valid.y\nxs = xs[to_keep]\nvalid_xs = valid_xs[to_keep]\nm = rf(xs, y)\nevaluate_model(m, xs, y, valid_xs, valid_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_raw.groupby('SaleCondition')['SalePrice'].mean().reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_raw.groupby('YrSold').agg(\n    avg_column2=('SalePrice', 'mean'),\n    count_column2=('Id', 'size')\n).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.groupby('SaleCondition').agg(\n#     avg_column2=('SalePrice', 'mean'),\n    count_column2=('Id', 'size')\n).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling","metadata":{}},{"cell_type":"code","source":"df = df_raw.copy()\ndf['SalePrice'] = np.log(df['SalePrice'])\ndf['GrLivArea'] = np.log(df['GrLivArea'])\ndf['YearBuilt'] = df['YearBuilt'] - df['YrSold']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"procs = [Categorify, FillMissing]\ncont,cat = cont_cat_split(df, 1, dep_var='SalePrice')\ncont.remove('OverallQual')\ncat.append('OverallQual')\nsplits = RandomSplitter(valid_pct=0.2, seed=42)(range_of(df))\ndls = TabularDataLoaders.from_df(df, procs=procs, cat_names=cat, cont_names=cont, y_names='SalePrice', splits=splits, bs=64)\nxs, y, valid_xs, valid_y = dls.train.xs, dls.train.y, dls.valid.xs, dls.valid.y\nm = rf(xs, y, n_estimators=500)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(m, xs, y, cv=5, scoring='neg_mean_squared_error')\nrmse_scores = np.sqrt(-scores)\n\nprint(\"Cross-validation RMSE scores:\", rmse_scores)\nprint(\"Mean RMSE:\", rmse_scores.mean())\nprint(\"Standard deviation of RMSE:\", rmse_scores.std())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\ndf2 = df.copy()\nimputer = KNNImputer(n_neighbors=5)\ndf2 = pd.get_dummies(df2, drop_first=True)\ndf2 = pd.DataFrame(imputer.fit_transform(df2), columns=df2.columns)\n\nX = df2.drop('SalePrice', axis=1)\ny = df2['SalePrice']\n\n\n\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 3: Define the hyperparameter grid\nparam_distributions = {\n    'n_estimators': [100, 200, 500, 1000],\n    'max_depth': [None, 10, 20, 30, 40],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2'],\n    'bootstrap': [True, False]\n}\n\n# Step 4: Set up the model and RandomizedSearchCV\nrf = RandomForestRegressor(random_state=42)\n\n# Use RandomizedSearchCV for faster tuning\nrandom_search = RandomizedSearchCV(\n    estimator=rf,\n    param_distributions=param_distributions,\n    n_iter=50,  # Number of parameter combinations to try\n    scoring='neg_mean_squared_error',\n    cv=3,  # 3-fold cross-validation\n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\n# Step 5: Fit the model to the training data\nrandom_search.fit(X_train, y_train)\n\n# Step 6: Get the best hyperparameters\nprint(f\"Best Parameters: {random_search.best_params_}\")\n\n# Step 7: Evaluate the model on validation set\nbest_rf = random_search.best_estimator_\ny_pred = best_rf.predict(X_val)\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\nprint(f\"Validation RMSE: {rmse}\")\n\n\n\n# Step 8: Evaluate the model's performance\n\n# Calculate performance metrics\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\nmae = mean_absolute_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\n\nprint(f\"Validation RMSE: {rmse}\")\nprint(f\"Validation MAE: {mae}\")\nprint(f\"Validation R²: {r2}\")\n\n# Step 9: Analyze Feature Importance\n\n# Get feature importance\nimportances = best_rf.feature_importances_\nfeature_names = X.columns\n\n# Create a DataFrame for better visualization\nimportances_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\nimportances_df = importances_df.sort_values(by='Importance', ascending=False)\n\n# Print top features\nprint(\"Top 10 Most Important Features:\")\nprint(importances_df.head(10))\n\n# Plot feature importance\nimportances_df.head(10).plot(kind='barh', x='Feature', y='Importance', legend=False)\nplt.title('Top 10 Most Important Features')\nplt.show()\n\n# Step 10: Residual Analysis\n\n# Calculate residuals\nresiduals = y_val - y_pred\n\n# Plot residuals\nplt.scatter(y_pred, residuals)\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"# plot_fi(rf_feat_importance(m_imp, xs_imp));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import seaborn as sns\n# import matplotlib.pyplot as plt\n# from scipy.cluster.hierarchy import linkage, leaves_list\n\n# def cluster_and_visualize_columns(df):\n#     # Step 1: Compute correlation matrix of the columns\n#     corr = df.corr()\n\n#     # Step 2: Perform hierarchical clustering on the correlation matrix\n#     Z = linkage(corr, method='ward')\n\n#     # Step 3: Get the column order based on the clustering\n#     col_order = leaves_list(Z)\n\n#     # Step 4: Reorder the correlation matrix\n#     clustered_corr = corr.iloc[col_order, col_order]\n\n#     # Step 5: Plot the clustered correlation matrix with better spacing\n#     plt.figure(figsize=(10, 8))  # Adjust figure size here\n#     sns.heatmap(clustered_corr, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 10},  # Adjust font size\n#                 cbar_kws={'shrink': 0.75})  # Adjust color bar size\n#     plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotate x-axis labels\n#     plt.yticks(fontsize=10)  # Adjust y-axis labels font size\n#     plt.title(\"Clustered Correlation Matrix\", fontsize=15)\n#     plt.tight_layout()  # Ensure everything fits without overlap\n#     plt.show()\n\n# cluster_and_visualize_columns(xs_imp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import warnings\n# warnings.simplefilter('ignore', FutureWarning)\n\n# from treeinterpreter import treeinterpreter\n# from waterfall_chart import plot as waterfall","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# row = valid_xs_imp.iloc[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction,bias,contributions = treeinterpreter.predict(m_imp, row.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction[0], bias[0], contributions[0].sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# waterfall(valid_xs_imp.columns, contributions[0], threshold=0.08, \n#           rotation_value=45,formatting='{:,.3f}');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"# df_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\").fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\").fillna(0)\n# df_test['GrLivArea'] = np.log(df['GrLivArea'])\n# dls_test = dls.test_dl(df_test)\n# xs_test = dls_test.xs\n# predictions,bias,contributions = treeinterpreter.predict(m, xs_test)\n# final_preds = np.exp(predictions)\n# test_ids = np.array(df_test['Id'])\n# submission_df = pd.DataFrame({\n#     'Id': test_ids,\n#     'SalePrice': final_preds[:, 0]\n# })\n\n# submission_df.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Improvement & Iteration","metadata":{}},{"cell_type":"markdown","source":"## 1. Impact of Financial Crisis","metadata":{}},{"cell_type":"code","source":"# priceYear = df.loc[:,['YrSold','SalePrice']]\n# avgPriceYear = priceYear.groupby('YrSold')['SalePrice'].mean()\n# avgPriceYear","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Its pretty clear that there was a break in trend around 2008, which coincides with the financial crisis and subsequent housing crash. Below, I will try to figure out exactly which month this break occurred.","metadata":{}},{"cell_type":"markdown","source":"### Partial Dependence","metadata":{}},{"cell_type":"code","source":"# from sklearn.inspection import PartialDependenceDisplay\n# PartialDependenceDisplay.from_estimator(m, valid_xs, ['YrSold','MoSold'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}