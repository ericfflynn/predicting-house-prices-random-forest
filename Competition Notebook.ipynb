{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment Setup","metadata":{}},{"cell_type":"code","source":"! pip install -Uqq fastbook waterfallcharts treeinterpreter dtreeviz==1.4.1\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nimport numpy\nimport pandas\nimport fastbook\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom treeinterpreter import treeinterpreter\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.inspection import PartialDependenceDisplay\nfastbook.setup_book()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Import & Exploration","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-09-12T14:18:38.390895Z","iopub.execute_input":"2024-09-12T14:18:38.391443Z","iopub.status.idle":"2024-09-12T14:18:38.905379Z","shell.execute_reply.started":"2024-09-12T14:18:38.391366Z","shell.execute_reply":"2024-09-12T14:18:38.904108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf_raw = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\nprint(\"df Shape: \",df_raw.shape)\nprint(df_raw.columns)\ndf_raw.describe()","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:17:32.757984Z","iopub.execute_input":"2024-09-12T23:17:32.758443Z","iopub.status.idle":"2024-09-12T23:17:32.912346Z","shell.execute_reply.started":"2024-09-12T23:17:32.758402Z","shell.execute_reply":"2024-09-12T23:17:32.910970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest with SciKit","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import RandomForestRegressor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_raw.copy()\ny = np.log(df['SalePrice'])\nX = df.drop('SalePrice',axis=1)\nprint(\"Total Features:\",len(X.columns))\nnumeric_features = X.select_dtypes(include=['number']).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\nprint(\"Numeric:\", len(numeric_features),\"Categorical\",len(categorical_features))","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:17:51.673295Z","iopub.execute_input":"2024-09-12T23:17:51.673719Z","iopub.status.idle":"2024-09-12T23:17:51.692071Z","shell.execute_reply.started":"2024-09-12T23:17:51.673679Z","shell.execute_reply":"2024-09-12T23:17:51.690787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"numeric_transformer = SimpleImputer(strategy='constant')\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\nmodel_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n])\n\nX_train, X_valid, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel_pipeline.fit(X_train, y_train)\ny_pred = model_pipeline.predict(X_valid)\n\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n\n#Gradient Boosting\nmodel_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', GradientBoostingRegressor(n_estimators=100, random_state=42))\n])\n\n\nX_train, x_valid, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel_pipeline.fit(X_train, y_train)\ny_pred = model_pipeline.predict(X_valid)\n\n\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(f\"Root Mean Squared Error (RMSE) with Gradient Boosting: {rmse}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-13T00:02:13.168440Z","iopub.execute_input":"2024-09-13T00:02:13.169233Z","iopub.status.idle":"2024-09-13T00:02:25.203356Z","shell.execute_reply.started":"2024-09-13T00:02:13.169175Z","shell.execute_reply":"2024-09-13T00:02:25.201994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"param_dist = {\n    'model__n_estimators': [100, 200, 300, 400],\n    'model__learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'model__max_depth': [3, 4, 5, 6],\n    'model__min_samples_split': [2, 5, 10],\n    'model__min_samples_leaf': [1, 2, 4],\n    'model__subsample': [0.8, 0.9, 1.0]\n}\n\nrandom_search = RandomizedSearchCV(\n    estimator=model_pipeline,\n    param_distributions=param_dist,\n    n_iter=20,  \n    cv=5,  \n    verbose=1,\n    random_state=42,\n    n_jobs=-1,  \n    scoring='neg_mean_squared_error'\n)\n\nrandom_search.fit(X_train, y_train)\n\n\nbest_model = random_search.best_estimator_\nprint(f\"Best parameters found: {random_search.best_params_}\")\n\ny_pred = best_model.predict(X_valid)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(f\"Root Mean Squared Error (RMSE) with tuned Gradient Boosting: {rmse}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-13T00:10:47.624480Z","iopub.execute_input":"2024-09-13T00:10:47.624971Z","iopub.status.idle":"2024-09-13T00:13:43.937120Z","shell.execute_reply.started":"2024-09-13T00:10:47.624923Z","shell.execute_reply":"2024-09-13T00:13:43.935678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\ny_pred_test = best_model.predict(X_test)\nsubmission = pd.DataFrame({\n    'Id': X_test['Id'], \n    'SalePrice': np.exp(y_pred_test)\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file 'submission.csv' created successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-13T00:14:58.727991Z","iopub.execute_input":"2024-09-13T00:14:58.729116Z","iopub.status.idle":"2024-09-13T00:14:58.808427Z","shell.execute_reply.started":"2024-09-13T00:14:58.729054Z","shell.execute_reply":"2024-09-13T00:14:58.807112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"df = df_raw.copy()\ndf['SalePrice'] = np.log(df['SalePrice'])\nprocs = [Categorify, FillMissing]\ncont,cat = cont_cat_split(df, 1, dep_var='SalePrice')\nsplits = RandomSplitter(valid_pct=0.3, seed=42)(range_of(df))\nprint(\"Continuous:\",len(cont), \"Categorical:\",len(cat))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = TabularDataLoaders.from_df(df, procs=procs, cat_names=cat, cont_names=cont, y_names='SalePrice', splits=splits, bs=64)\nprint(\"Train:\", len(dls.train.items), \"Valid:\", len(dls.valid.items))\ndls.show(3)\ndls.items.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Model","metadata":{}},{"cell_type":"code","source":"def rf(xs, y, n_estimators=250, max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_features=max_features,\n                                min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs, y, valid_xs, valid_y = dls.train.xs, dls.train.y, dls.valid.xs, dls.valid.y\nm = rf(xs, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"def r_mse(pred,y): \n    return round(math.sqrt(((pred-y)**2).mean()), 6)\n\ndef m_rmse(m, xs, y): \n    return r_mse(m.predict(xs), y)\n\ndef evaluate_cross_val(m, xs, y):\n    scores = cross_val_score(m, xs, y, cv=5, scoring='neg_mean_squared_error')\n    rmse_scores = np.sqrt(-scores)\n    print(\"Cross-validation RMSE scores:\", rmse_scores)\n    print(\"Mean RMSE:\", rmse_scores.mean())\n    print(\"Standard deviation of RMSE:\", rmse_scores.std())\n    \ndef evaluate_model(m, xs, y, valid_xs, valid_y):\n    print(\"Training Set RMSE:\",m_rmse(m, xs, y))\n    evaluate_cross_val(m, xs, y)\n    print(\"_______________________________________________________________\")\n    print(\"OOB Error:\",r_mse(m.oob_prediction_, y))\n    print(\"Validation Set RMSE:\",m_rmse(m, valid_xs, valid_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(m, xs, y, valid_xs, valid_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Model Imputation","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Evaluate Feature Importance","metadata":{}},{"cell_type":"code","source":"def rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi[fi['cols'] == 'SaleCondition']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi = rf_feat_importance(m, xs)\n\nplot_fi(fi[:21]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_keep = fi[fi.imp>0.0045].cols\nlen(to_keep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs = xs[to_keep]\nvalid_xs = valid_xs[to_keep]\nm = rf(xs, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(m, xs, y, valid_xs, valid_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Investigate Most Important Features","metadata":{}},{"cell_type":"markdown","source":"#### OverallQual (Overall material and finish quality)\nChanges Attempted:\n1. Change Cont - Cat\n2. Square value\n3. Custom bins\n","metadata":{}},{"cell_type":"code","source":"df = df_raw.copy()\ndf['SalePrice'] = np.log(df['SalePrice'])\ndf['OverallQual_cat'] = pd.cut(df['OverallQual'], bins=[0,4,7,10], labels=['Poor', 'Ok','Great'])\nprocs = [Categorify, FillMissing]\ncont,cat = cont_cat_split(df, 1, dep_var='SalePrice')\nsplits = RandomSplitter(valid_pct=0.2, seed=42)(range_of(df))\ndls = TabularDataLoaders.from_df(df, procs=procs, cat_names=cat, cont_names=cont, y_names='SalePrice', splits=splits, bs=64)\nxs, y, valid_xs, valid_y = dls.train.xs, dls.train.y, dls.valid.xs, dls.valid.y\nm = rf(xs, y)\nevaluate_model(m, xs, y, valid_xs, valid_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['OverallQual'].hist()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PartialDependenceDisplay.from_estimator(m, valid_xs, ['OverallQual']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### GrLivingArea (Above ground square footage)","metadata":{}},{"cell_type":"code","source":"df['GrLivArea'].min(), df['GrLivArea'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['GrLivArea'].hist(bins=30)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PartialDependenceDisplay.from_estimator(m, valid_xs, ['GrLivArea']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Year Built","metadata":{}},{"cell_type":"code","source":"df['YearBuilt'].hist(bins= 20)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PartialDependenceDisplay.from_estimator(m, valid_xs, ['YearBuilt','YearRemodAdd']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['YearRemodAdd'].max(), df['YearRemodAdd'].min())\nprint(df['YearRemodAdd'][df['YearRemodAdd'].isnull()])\ndf['YearRemodAdd'].hist(bins= 50)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_raw.copy()\ndf['SalePrice'] = np.log(df['SalePrice'])\ndf['Score'] = (df['OverallQual']*df['OverallCond'])\nprocs = [Categorify, FillMissing]\ncont,cat = cont_cat_split(df, 1, dep_var='SalePrice')\nsplits = RandomSplitter(valid_pct=0.2, seed=42)(range_of(df))\ndls = TabularDataLoaders.from_df(df, procs=procs, cat_names=cat, cont_names=cont, y_names='SalePrice', splits=splits, bs=64)\nxs, y, valid_xs, valid_y = dls.train.xs, dls.train.y, dls.valid.xs, dls.valid.y\nxs = xs[to_keep]\nvalid_xs = valid_xs[to_keep]\nm = rf(xs, y)\nevaluate_model(m, xs, y, valid_xs, valid_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_raw.groupby('SaleCondition')['SalePrice'].mean().reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_raw.groupby('YrSold').agg(\n    avg_column2=('SalePrice', 'mean'),\n    count_column2=('Id', 'size')\n).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.groupby('SaleCondition').agg(\n#     avg_column2=('SalePrice', 'mean'),\n    count_column2=('Id', 'size')\n).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling","metadata":{}},{"cell_type":"code","source":"df = df_raw.copy()\ndf['SalePrice'] = np.log(df['SalePrice'])\ndf['GrLivArea'] = np.log(df['GrLivArea'])\ndf['YearBuilt'] = df['YearBuilt'] - df['YrSold']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"procs = [Categorify, FillMissing]\ncont,cat = cont_cat_split(df, 1, dep_var='SalePrice')\ncont.remove('OverallQual')\ncat.append('OverallQual')\nsplits = RandomSplitter(valid_pct=0.2, seed=42)(range_of(df))\ndls = TabularDataLoaders.from_df(df, procs=procs, cat_names=cat, cont_names=cont, y_names='SalePrice', splits=splits, bs=64)\nxs, y, valid_xs, valid_y = dls.train.xs, dls.train.y, dls.valid.xs, dls.valid.y\nm = rf(xs, y, n_estimators=500)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(m, xs, y, cv=5, scoring='neg_mean_squared_error')\nrmse_scores = np.sqrt(-scores)\n\nprint(\"Cross-validation RMSE scores:\", rmse_scores)\nprint(\"Mean RMSE:\", rmse_scores.mean())\nprint(\"Standard deviation of RMSE:\", rmse_scores.std())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\ndf2 = df.copy()\nimputer = KNNImputer(n_neighbors=5)\ndf2 = pd.get_dummies(df2, drop_first=True)\ndf2 = pd.DataFrame(imputer.fit_transform(df2), columns=df2.columns)\n\nX = df2.drop('SalePrice', axis=1)\ny = df2['SalePrice']\n\n\n\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 3: Define the hyperparameter grid\nparam_distributions = {\n    'n_estimators': [100, 200, 500, 1000],\n    'max_depth': [None, 10, 20, 30, 40],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2'],\n    'bootstrap': [True, False]\n}\n\n# Step 4: Set up the model and RandomizedSearchCV\nrf = RandomForestRegressor(random_state=42)\n\n# Use RandomizedSearchCV for faster tuning\nrandom_search = RandomizedSearchCV(\n    estimator=rf,\n    param_distributions=param_distributions,\n    n_iter=50,  # Number of parameter combinations to try\n    scoring='neg_mean_squared_error',\n    cv=3,  # 3-fold cross-validation\n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\n# Step 5: Fit the model to the training data\nrandom_search.fit(X_train, y_train)\n\n# Step 6: Get the best hyperparameters\nprint(f\"Best Parameters: {random_search.best_params_}\")\n\n# Step 7: Evaluate the model on validation set\nbest_rf = random_search.best_estimator_\ny_pred = best_rf.predict(X_val)\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\nprint(f\"Validation RMSE: {rmse}\")\n\n\n\n# Step 8: Evaluate the model's performance\n\n# Calculate performance metrics\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\nmae = mean_absolute_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\n\nprint(f\"Validation RMSE: {rmse}\")\nprint(f\"Validation MAE: {mae}\")\nprint(f\"Validation RÂ²: {r2}\")\n\n# Step 9: Analyze Feature Importance\n\n# Get feature importance\nimportances = best_rf.feature_importances_\nfeature_names = X.columns\n\n# Create a DataFrame for better visualization\nimportances_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\nimportances_df = importances_df.sort_values(by='Importance', ascending=False)\n\n# Print top features\nprint(\"Top 10 Most Important Features:\")\nprint(importances_df.head(10))\n\n# Plot feature importance\nimportances_df.head(10).plot(kind='barh', x='Feature', y='Importance', legend=False)\nplt.title('Top 10 Most Important Features')\nplt.show()\n\n# Step 10: Residual Analysis\n\n# Calculate residuals\nresiduals = y_val - y_pred\n\n# Plot residuals\nplt.scatter(y_pred, residuals)\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"# plot_fi(rf_feat_importance(m_imp, xs_imp));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import seaborn as sns\n# import matplotlib.pyplot as plt\n# from scipy.cluster.hierarchy import linkage, leaves_list\n\n# def cluster_and_visualize_columns(df):\n#     # Step 1: Compute correlation matrix of the columns\n#     corr = df.corr()\n\n#     # Step 2: Perform hierarchical clustering on the correlation matrix\n#     Z = linkage(corr, method='ward')\n\n#     # Step 3: Get the column order based on the clustering\n#     col_order = leaves_list(Z)\n\n#     # Step 4: Reorder the correlation matrix\n#     clustered_corr = corr.iloc[col_order, col_order]\n\n#     # Step 5: Plot the clustered correlation matrix with better spacing\n#     plt.figure(figsize=(10, 8))  # Adjust figure size here\n#     sns.heatmap(clustered_corr, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 10},  # Adjust font size\n#                 cbar_kws={'shrink': 0.75})  # Adjust color bar size\n#     plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotate x-axis labels\n#     plt.yticks(fontsize=10)  # Adjust y-axis labels font size\n#     plt.title(\"Clustered Correlation Matrix\", fontsize=15)\n#     plt.tight_layout()  # Ensure everything fits without overlap\n#     plt.show()\n\n# cluster_and_visualize_columns(xs_imp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import warnings\n# warnings.simplefilter('ignore', FutureWarning)\n\n# from treeinterpreter import treeinterpreter\n# from waterfall_chart import plot as waterfall","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# row = valid_xs_imp.iloc[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction,bias,contributions = treeinterpreter.predict(m_imp, row.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction[0], bias[0], contributions[0].sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# waterfall(valid_xs_imp.columns, contributions[0], threshold=0.08, \n#           rotation_value=45,formatting='{:,.3f}');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"# df_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\").fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\").fillna(0)\n# df_test['GrLivArea'] = np.log(df['GrLivArea'])\n# dls_test = dls.test_dl(df_test)\n# xs_test = dls_test.xs\n# predictions,bias,contributions = treeinterpreter.predict(m, xs_test)\n# final_preds = np.exp(predictions)\n# test_ids = np.array(df_test['Id'])\n# submission_df = pd.DataFrame({\n#     'Id': test_ids,\n#     'SalePrice': final_preds[:, 0]\n# })\n\n# submission_df.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Improvement & Iteration","metadata":{}},{"cell_type":"markdown","source":"## 1. Impact of Financial Crisis","metadata":{}},{"cell_type":"code","source":"# priceYear = df.loc[:,['YrSold','SalePrice']]\n# avgPriceYear = priceYear.groupby('YrSold')['SalePrice'].mean()\n# avgPriceYear","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Its pretty clear that there was a break in trend around 2008, which coincides with the financial crisis and subsequent housing crash. Below, I will try to figure out exactly which month this break occurred.","metadata":{}},{"cell_type":"markdown","source":"### Partial Dependence","metadata":{}},{"cell_type":"code","source":"# from sklearn.inspection import PartialDependenceDisplay\n# PartialDependenceDisplay.from_estimator(m, valid_xs, ['YrSold','MoSold'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}